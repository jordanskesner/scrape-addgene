# Installs
# pip install selenium

# ChromeDriver download
# https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win64/chromedriver-win64.zip

# Imports
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time
import re
import sqlite3
from pathlib import Path


# Initialize driver 
# Specify the path to your chromedriver
#driver_path = "C://Users//jsk33//OneDrive//Github//scrape-addgene//chromedriver-win64//chromedriver.exe"  # Replace with the actual path to the WebDriver
driver_path = ".//chromedriver-win64//chromedriver.exe"
service = Service(driver_path)

# Initialize WebDriver
#driver = webdriver.Chrome(service=service)


# create database if doesnt exist
# instantiate

# path to database file
db_path = Path("addgene_database.db")

# check if file exists, ifnot create it
if not file_path.is_file():
    print(f"File '{file_path}' does not exist. Creating database file...")
    conn = sqlite3.connect('example.db')
else:
    print(f"File '{file_path}' already exists.")
    conn = sqlite3.connect('example.db')

#
cursor = conn.cursor()

# Create table
cursor.execute('''CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)''')

# Insert a row
cursor.execute('INSERT INTO users (name, age) VALUES (?, ?)', ('Alice', 25))

# Query data
cursor.execute('SELECT * FROM users')
print(cursor.fetchall())

conn.commit()
conn.close()









#
try:

    # Initialize WebDriver
    driver = webdriver.Chrome(service=service)
    
    # Open the target website
    url = 'https://www.addgene.org/'  # Replace with the website you want to scrape
    driver.get(url)
    
    # Wait for the page to load
    time.sleep(2)  # Adjust based on the website's loading speed
 
    # Locate the search bar element (e.g., by name, id, class, or CSS selector)
    # <input form="search-bar" id="search-text-input" class="suggest-input" aria-expanded="false" aria-haspopup="listbox" type="text" role="combobox" autocomplete="off" placeholder="e.g. 74218, Cas9, transformation protocol" name="q" aria-owns="awesomplete_list_2">
    search_bar = driver.find_element(By.ID, 'search-text-input')  # Replace 'q' with the name of the search input field
    
    # Enter text into the search bar
    search_text = "GFP"  # Replace with the text you want to enter
    search_bar.send_keys(search_text)
    
    # Submit the search (if needed, e.g., by pressing Enter)
    search_bar.send_keys(Keys.RETURN)
    
    # Wait for the results to load
    time.sleep(2)

    # Select the plasmids subcategory
    inplasmids_bar = driver.find_element(By.XPATH, "//span[@class='leaf-label' and text()='in Plasmids']")
    inplasmids_bar.click() # click it

    # Wait for the results to load
    time.sleep(2)

    # <a href="/185404/">GFP1-10-miniCMV-GFP11×11-GFP-tDeg</a>
    # Find all elements with an 'href' attribute
    elements_with_href = driver.find_elements(By.XPATH, "//a[@href]")
    # Extract the href values and store them in a list
    href_list = [element.get_attribute('href') for element in elements_with_href]
    # Print the list of hrefs
    # print("List of hrefs:", href_list)

    # filter the useful href
    # Regular expression to match only URLs starting with 'https://'
    regex = r"addgene.org/\d\d\d\d\d{1,2}/"
    # Filter the list using the regex
    filtered_items = [item for item in href_list if re.search(regex, item)]
    # Print the filtered list
    print("Number of filtered items:", len(filtered_items), "\n")
    print("Filtered items:", filtered_items, "\n")

    # target page at this step should be showing 20 plasmids per page, but we are getting returned only 14 IDs after regex
    # manually constructing list of hrefs from first page to determine which are missing
    # <a href="/185404/">GFP1-10-miniCMV-GFP11×11-GFP-tDeg</a>
    # <a href="/87906/">pENTR221-H1-sgGFP1-U6-sgGFP2-7SK-sgGFP3</a>
    # <a href="/133815/">pFSW GFP IRES GFP</a>
    # After first 3 entries, it appears the ones missed have 5 digits, not 6. updating regex

    # Pull numbers from the filtered strings
    # Remove everything except digits from each string
    clean_IDs = [re.sub(r"\D", "", item) for item in filtered_items]
    # Print the result
    print("Cleaned IDs", clean_IDs, "\n")

    # navigate to the first plasmid
    # Select the first plasmids
    plasmid_ID = clean_IDs[0]
    print("First ID", plasmid_ID, "\n")
    firstplasmid_bar = driver.find_element(By.XPATH, f"//*[@href='/{plasmid_ID}/']")
    firstplasmid_bar.click() # click it

    # Wait for the results to load
    time.sleep(5)

    # click on sequences
    # <a href="/185404/sequences/"> Sequences (1) </a>
    sequences_bar = driver.find_element(By.XPATH, "//text()='Sequences']")
    sequences_bar.click()
    time.sleep(5)

#
finally:
    # Close the WebDriver
    driver.quit()






<span class="leaf-label">in Plasmids</span>





try:
    # Open the target website
    url = 'https://example.com'  # Replace with the website you want to scrape
    driver.get(url)
    
    # Wait for the page to load
    time.sleep(2)
    
    # Example: Find an element by its tag name
    links = driver.find_elements(By.TAG_NAME, 'a')
    for link in links:
        print(link.text, link.get_attribute('href'))
    
finally:
    # Close the WebDriver
    driver.quit()

